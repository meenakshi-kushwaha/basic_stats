---
title: "Statistical Concepts"
subtitle: "Using some artwork"
author: "Meenakshi Kushwaha"
institute: "ILK Labs, Bangalore"
date: "2020/06/13 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader
---
class:center, middle

# Artwork by @allison_horst
## https://github.com/allisonhorst/stats-illustrations


---
name:index
## [Session 1](#Session1): Introduction
## [Session 2](#Session2): Simple Linear Regressin & Diagnostic plots
## [Session 3](#Session3): Deming Regression
## [Session 4](#Session4): Moving Averages 
## [Session 5](#Session5): Normal Distribution
## [Session 6](#Session6): Standard Error
## [Session 7](#Session7): Data Transformations

???
hyperlink the slides
---
class:middle, center
name: Session1
# Session1
## 26th June 2020
## t test, p values, and multiple linear regression with dragons!
---
#p-value
![pvalue0](images/pvalue5.jpeg)
---
#p-value
![pvalue6](images/pvalue6.jpeg)
---
#p-value
![pvalue7](images/pvalue7.jpeg)
---
#p-value

![pvalue1](images/pvalue1.jpeg)
---
#p-value
![pvalue1](images/pvalue2.jpeg)
---
#p-value
![pvalue1](images/pvalue3.jpeg)
---
#p-value
![pvalue4](images/pvalue4.jpeg)

---
# (Simple) Linear Regression
- Dependent variable (y)
- Independent variable (x)  


![scatter](images/scatter.png)




---
# Multiple linear regression

### Instead of, x---->y
   - Example : RH---->PM2.5

### x1, x2, x3.... ---->y
  - Example: RH, Temperature, Season, Wind speed ----> PM2.5

Assumption?

---
class:middle, center
# Dependent variable (y) should be continous
---
class:middle, center
# Multiple linear regression


![dragons](images/dragons.png)

---
class:center, middle
Interpret coefficients for continous predictor variables

![continous variables](images/dragons_continuous.png)
---

class:middle, center
Categorical variables

![categorical variables](images/dragon_regression.png)

---

#Making predictions
![prediction](images/dragon_predict_mlr.png)
---
# Residuals
![residuals](images/dragon_residual.png)
---
# Are residuals normal?

![residual distribution](images/dragon_residual_distribution.png)

---
class:middle, center
# End of Session 1
---
class: middle, center
name: Session2
# Session 2
### Simple Linear Regression
### 4th July 2020
---
# Simple Linear Regression

- We referred to slides from Allison Horst https://bit.ly/2NXHtUD
  - slides 11-41
- We covered how to do diagnostics on simple linear regression models
- In R use `plot()` function to generate diagonostic plots
- QQ plot https://www.youtube.com/watch?v=okjYjClSjOg


### !!! ALWAYS look at your data !!!
---
class:middle, center
# End of Session 2
---
name: Session3
# Session 3
### Deming Regression
### 11th July 2020




---
# Assumptions for Simple Linear Regression 
- X axis is free of error
- We regress Y on X for callibration equation
- We use X to predict Y
- What if both X and Y have some error
  - Example, Atmos vs Purple air
  - i.e. we can't consider X as a reference
  
---

![recap](images/ah_residual.png)



Source: Allison Horst Lectures
---
 
![deming1](images/deming1.png)


.footnote[AAAR-Volken-Apte-Tutorial]

---
# Deming regression in R
- Use packages `deming` or `mcr`
- If both X and Y are subject to different analytical variability we need to provide an error ratio for X vs Y


---
class:middle, center
# End of Session 3
---
name: Session4
#Sesson 4 
### Moving averages
### 17th July 2020

![ma cartoon](images/ma_cartoon.png)

.footnote[source:https://www.storybench.org/how-to-calculate-a-rolling-average-in-r/ ]
---
# Moving averages
### Rolling or moving averages are a way to reduce noise in time series data
- Type of smoothing technique
- Every value is smoothed by incorporating information from data in adjacent timepoints
- Other smoothing techniques
  - local regression (loess)
---
![MA illustration](images/39-rolling-averages.png)

---
# How do I?
- Specify the window = no. of points to be averaged 
  - 7 day moving average
  - 8 hour moving average
- which time window to use is a judgment call
  - you want more clarity without losing important data
- Some variations
  - centered vs trailing
---
# openair Package
Use function `rollingMean()`
```r

rollingMean(mydata, pollutant = "PM10", hours = 8) 

```

---
![fl](images/fl_new_cases.png)
---
class: middle, center
![facet](images/fct_us_cvd.png)
---
# Smoothing using `loess`
- local regression, default method for `ggplot()`
--
![loess](images/loess-animation.gif)
.footnote[Introduction to data science, e-book by *Rafael A. Irizarry*]
---

class: middle, center
#End of Session 4

.footnote[Deep dive:
http://www2.stat.duke.edu/~banks/218-lectures.dir/dmlect2.pdf]
---
name: Session
#Session 5 
24th July 2020
### Normal Distribution 
![teacups](images/teacups.png)
.footnote[https://tinystats.github.io/teacups-giraffes-and-statistics]
---
# Collect Data
![measure](images/measure_giraffe.png)
![island](images/island.png)
---
#Visualize the data
https://tinystats.github.io/teacups-giraffes-and-statistics/02_bellCurve.html
--

![histogram](images/hist.png)
---
.pull-left[
# Distribution
- All the values in your dataset
- How often each value occurs
- Shape, center, and amount of variability in the data
]
--
.pull-right[
# Normal Disribution
- There is a single peak
- The mass of the distribution is at its center
- There is symmetry about the center line
]
--
![normal disribution](images/normal_dist.png)
---
#Mean, Median, Mode
###Measures of centrality
When the data are normally distributed the median and the mean will be very close to each other

When your data are **not normally** distributed the **median** is a more appropriate measure of centrality  
--

.center[
![median](images/median.gif)
]
---
# Calculating the mean
\begin{equation}
 \tag{1}
 \Large{\bar{x}} = \frac{x_1 + x_2 + ... + x_n}{n}
 \end{equation}  
 

 \begin{equation}
 \tag{2}
 \Large{\bar{x}} = \frac{\sum_{i=1}^{n}{x_i}}{n}
 \end{equation}

--
 ![equation](images/equation.png)
---
#Measure of spread
![spread](images/Range.png)
"If we want to avoid undue influence of outliers for the measure of spread, the range is not good enough to provide us with a wholistic, robust measure."
---
#Variance
![bells_variance](images/bells_variance.png)

"variance gives us an idea of how similar observations are to one another, and to the average value"
---
# Calculating population variance

- Deviation: Distance from mean (x-mu)
- Sum of squares
- Variance 
![variance](images/variance.png)
- Standard Deviation
![sd](images/sd.png)

---
# Sum of squares
![sumofsquares](images/sumofsquares.gif)
---
# Sum of squares
![sumofsquares2](images/sumofsquares2.png)
![squares](images/Squares1.png)
---
# Calculating Sample variance

\begin{equation} 
 \tag{variance}
 \Large s^2 = \frac{\sum_{i=1}^n (x_i - \bar{x})^2}{n-1}
 \end{equation}    
 
 \begin{equation}
 \tag{standard deviation}
 \Large s = \sqrt{\frac{\sum_{i=1}^n (x_i - \bar{x})^2}{n-1}}
 \end{equation}
 
---
#Meaning of standard deviation
![emperical rule](images/empirical_rule.jpg)
- The interval created by **1 standard deviation above and below the mean includes 68%** of all the data.

- **2 standard deviations** above and below the mean encompasses approximately **95%** of the data.
- **3 standard deviations** above and below the mean encompass **99.7%** of the data.
---
# Example
![giraffe_Variance3](images/giraffe_variance3.jpg) 
---
#Application
### How probable it is to find a giraffe smaller than our favorite purple-spotted one?
- ~1 standard deviation below the mean

- assuming normal distribution
**what percentage of giraffes will be shorter than the one with purple spots?**

--
![application](images/Empirical_example.png)


---
class: middle, center
# Homework
### We have to correct the calculated variance by dividing by n−1. Why?
--

Answer - https://tinystats.github.io/teacups-giraffes-and-statistics/04_variance.html

---
class:center
# End of Session 5
![marmalade](images/Marmalade.png)
---
name: Session6
# Session 6
### 3rd Dec 2020
### Standard Error
.footnote[https://tinystats.github.io/teacups-giraffes-and-statistics]

#### Is the average height of a giraffe on island 1 less than 11cm?
---
#Recap
![](images/island.png)
![](images/measure_giraffe.png)
---
#Recap - Emperical rule
![](images/empirical_rule.jpg)
- The entire normal distribution includes 100% of the data.  **1 standard deviation above and below the mean includes 68% of all the data**. 

- **2 standard deviations above and below the mean encompasses approximately 95% of the data**. 

- **3 standard deviations above and below the mean encompass 99.7% of the data**, capturing almost all possible observations in the set. 
---
# Sample size and mean 
- Not all sample means are created equal. 
- Some are better estimates than others.
![](images/unnamed-chunk-3-.gif)
Why do we care about accuracy of a sample estimate?
???
because we want to be able to answer questions about the population by making inferences. Statistical inference uses math to draw conclusions about the population based on a subset of the full picture (i.e. a sample). Subsets of data are of course limited, so it’s therefore important to acknowledge that the strength of the conclusions drawn about the population is dependent on the precision of the sample estimate. 
---
![](images/Notebooks.jpg)
---
# Sampling distribution
![](images/sampling_distribution.png)
--
The range of possible mean values are between 8.9 and 10.7. This means mean values outside of this range are essentially improbable.
--
---
# Sample size
### What if we sample 500 giraffes instead of 50?

.pull-left[
![](images/sample_size.png)
]

--

.pull-right[

- All histograms look normal.

- All distributions have approximately the same mean.

- Higher sample sizes --> lesser disperstion or range of mean values

]

--

---
class: center

## Summarize the spread of the mean in a single value

--

### Standard error

--

Standard deviation of a sampling distribution.  

--
  
Here “standard error” will imply standard error of the mean. But we can technically calculate the standard error of any sample statistic, not just the mean.

--
---
# Calculating Standard error of the mean (SEM)

https://tinystats.github.io/teacups-giraffes-and-statistics/06_standardError.html#standard_error_in_practice
---
# Confidence Intervals 
Remember, that the SEM is just the standard deviation of the sampling distribution, so we can apply the empirical rule. As a result, ± 2 SE from a point estimate will capture ~95% of the sampling distribution (exactly, 95.45% of the data). The true value is 1.96 standard deviations–and this is what we use to construct a 95% confidence interval (CI).
![](images/General_empirical.jpg)
---
# Confidence Intervals

### If we were to sample over and over again, then 95% of the time the CIs would contain the true mean.

Importantly, some examples of what the 95% CI does NOT mean are:

--

- A 95% CI does not mean that it contains 95% of the sample data.

--

- A CI is not a definitive range of likely values for the sample statistic, but you can think of it as estimate of likely values for the population parameter.

--

- It does not mean that values outside of the 95% CI have a 5% chance of being the true mean.

--

---
# Confidence Intervals
.pull-left[
![](images/ci1.jpg)
]

--

.pull-right[
![](images/ci2.jpg)
]

.footnote[https://medium.com/@EpiEllie/having-confidence-in-confidence-intervals-8f881712d837]
---
class:center
![](images/BabyInference.jpg)
---
name: Session7
# Session 7
# Data Transformations
- ###Log Transformation
- ###Data Normalization

.footnote[Source:https://online.stat.psu.edu/stat501/lesson/9]
---
class:inverse
#Log Transformation
- ###What is log transformation
- ###What are we transforming?
---
# Log transformation
- Natural logarithm 
  - Denoted by *ln*, *log<sub>e*, or *log* where *e* is the base
  - where e is the constant 2.718...
  - For example, the natural logarithm of 5 is the power to which you have to raise e = 2.718282... in order to get 5. Notationally, *ln*(5)=1.60944
  
  
- A logarithm can have different bases (*e*, 2, 10, etc) 
  - For example  *log<sub>10*(1000) = 3
  - 10 raised to the power 3 is 1000
---
#Log transforming Predictor variable
- 13 subjects were asked to memorize a list of disconnected items
- The subjects were then asked to recall the items at various times
- The proportion of items (y = prop) correctly recalled at various times (x = time, in minutes)


![](images/dt1.png)
---
### No linear fit. Below is residuals plot
![](images/dt2.png)
---
#Log transforming Predictor variable
.pull-left[
![](images/dt_Table.png)
]

--

.pull-right[
![](images/dt4.png)
]
---
#Log transforming outcome variable
- Relationship between birthweight and length of gestation for various mammals
- Birthweight (x, in kg) is the predictor and the length of gestation (y, in number of days until birth) is the response.
![](images/dt5.png)
---
### no linear relationship. Residuals plot below
![](images/dt6.png)
---
#Log transforming outcome variable
![](images/dt_Table2.png)

---
#Log transforming outcome variable

![](images/dt7.png)

---
class:inverse
#Data Normalization
### For comparison across different studies/variables
### How are we normalizing the data?
---
# Few ways to normalize
- Divide by mean
- Divide by IQR
- Divide by Standard deviation

- NOTE
  - Mention in your analysis and plots if your data is normalized
  - Keep in mind the transformation when interpreting your results
  - Normalizing can mean different things in different contexts. Often normalization, rescaling and standardization may be used interchangeably.
  
- Further Reading -  TBD 
  
---
class:inverse, middle, center
# End of Session 7
### See you in 2021 :)
---


class:middle, center

![you can do it](images/code_hero_rstats.png)
---
